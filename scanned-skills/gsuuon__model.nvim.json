[
  {
    "name": "model.nvim (gsuuon)",
    "description": "[é¡¹ç›®çº§] Neovim plugin for interacting with LLM's and building editor integrated prompts.ã€‚æ¥æº: https://github.com/gsuuon/model.nvim",
    "domain": "ai-llm",
    "tags": [
      "ai",
      "chatgpt",
      "llm",
      "neovim",
      "neovim-plugin",
      "lua",
      "stars-396",
      "github-gsuuon",
      "project-level"
    ],
    "genes": [
      {
        "title": "é¡¹ç›®æ¦‚è¿°",
        "content": "Neovim plugin for interacting with LLM's and building editor integrated prompts.\n\næ¥æº: https://github.com/gsuuon/model.nvim\nè¯­è¨€: Lua\nStars: 396",
        "gene_type": "principle"
      },
      {
        "title": "å®‰è£…ä¸å¿«é€Ÿå¼€å§‹",
        "content": "",
        "gene_type": "snippet"
      },
      {
        "title": "é¡¹ç›®ç»“æ„",
        "content": "LICENSE\nMakefile\nREADME.md\nTODO.md\nlua\nlua/model\nlua/telescope\nlua/tests\nplugin\nplugin/init.lua\npython3\npython3/Pipfile\npython3/Pipfile.lock\npython3/store.py\nqueries\nqueries/mchat\nselene.toml\nsyntax\nsyntax/mchat.vim\ntests",
        "gene_type": "pattern"
      },
      {
        "title": "ä½¿ç”¨æ–¹æ³•",
        "content": "https://github.com/gsuuon/model.nvim/assets/6422188/ae00076d-3327-4d97-9cc1-41acffead327\n\n\n**model.nvim** comes with some [starter prompts](./lua/model/prompts/starters.lua) and makes it easy to build your own prompt library. For an example of a more complex agent-like multi-step prompt where we curl for openapi schema, ask gpt for relevant endpoint, then include that in a final prompt look at the `openapi` starter prompt.\n\nPrompts can have 5 different [modes](#segmentmode) which determine what happens to the response: append, insert, replace, buffer, insert_or_replace. The default is to appen",
        "gene_type": "snippet"
      },
      {
        "title": "æ ¸å¿ƒç‰¹æ€§",
        "content": "- ğŸª Provider agnostic. Comes with:\n  - hosted\n    - OpenAI ChatGPT (and compatible API's)\n    - Google PaLM, together, huggingface\n  - local\n    - llama.cpp\n    - ollama\n  - easy to add your own\n- ğŸ¨ Programmatic prompts in lua\n  - customize everything\n  - async and multistep prompts\n  - starter examples\n- ğŸŒ  Streaming completions\n  - directly in buffer\n  - transform/extract text\n  - append/replace/insert modes\n- ğŸ¦œ Chat in `mchat` filetype buffer\n  - edit settings or messages at any point\n  - take conversations to different models\n  - treesitter highlights and folds",
        "gene_type": "checklist"
      }
    ],
    "version": 1,
    "usage_count": 0,
    "rating": 4.2988475929627565
  },
  {
    "name": "model.nvim/lua æ¨¡å—",
    "description": "[æ¨¡å—çº§] model.nvim é¡¹ç›®çš„ lua æ¨¡å— (source)ã€‚æ¥æº: https://github.com/gsuuon/model.nvim",
    "domain": "ai-llm",
    "tags": [
      "ai",
      "chatgpt",
      "llm",
      "neovim",
      "neovim-plugin",
      "module-level",
      "source",
      "lua"
    ],
    "genes": [
      {
        "title": "lua æ¨¡å—è¯´æ˜",
        "content": "```lua\n--- Looks for `<llm:` at the end and splits into before and after\n--- returns all text if no directive\nlocal function match_llm_directive(text)\n  local before, _, after = text:match(\"(.-)(<llm:)%s?(.*)$\")\n  if not before and not after then\n    before, after = text, \"\"\n  elseif not before then\n    before = \"\"\n  elseif not after then\n    after = \"\"\n  end\n  return before, after\nend\n\nlocal instruct_code = 'You are a highly competent programmer. Include only valid code in your response.'\n\nretu",
        "gene_type": "principle"
      },
      {
        "title": "lua æ–‡ä»¶ç»“æ„",
        "content": "lua/model\nlua/model/core\nlua/model/core/chat.lua\nlua/model/core/input.lua\nlua/model/core/provider.lua\nlua/model/core/scopes.lua\nlua/model/format\nlua/model/format/claude.lua\nlua/model/format/llama2.lua\nlua/model/format/starling.lua\nlua/model/format/zephyr.lua\nlua/model/format/zephyr_tokenizing.lua\nlua/model/init.lua\nlua/model/prompts\nlua/model/prompts/chats.lua",
        "gene_type": "pattern"
      },
      {
        "title": "lua ä»£ç ç¤ºä¾‹",
        "content": "--- Looks for `<llm:` at the end and splits into before and after\n--- returns all text if no directive\nlocal function match_llm_directive(text)\n  local before, _, after = text:match(\"(.-)(<llm:)%s?(.*)$\")\n  if not before and not after then\n    before, after = text, \"\"\n  elseif not before then\n    before = \"\"\n  elseif not after then\n    after = \"\"\n  end\n  return before, after\nend\n\nlocal instruct_code = 'You are a highly competent programmer. Include only valid code in your response.'\n\nreturn {\n  ['to code'] = {\n    provider = openai,\n    builder = function(input)\n      local text, directive = m",
        "gene_type": "snippet"
      }
    ],
    "version": 1,
    "usage_count": 0,
    "rating": 3.539078074370205
  },
  {
    "name": "model.nvim/python3 æ¨¡å—",
    "description": "[æ¨¡å—çº§] model.nvim é¡¹ç›®çš„ python3 æ¨¡å— (source)ã€‚æ¥æº: https://github.com/gsuuon/model.nvim",
    "domain": "ai-llm",
    "tags": [
      "ai",
      "chatgpt",
      "llm",
      "neovim",
      "neovim-plugin",
      "module-level",
      "source",
      "python3"
    ],
    "genes": [
      {
        "title": "python3 æ¨¡å—è¯´æ˜",
        "content": "æ¨¡å—è·¯å¾„: python3/\nç±»å‹: source\næ–‡ä»¶æ•°: 3\n\nåŒ…å«æ–‡ä»¶:\n- python3/Pipfile\n- python3/Pipfile.lock\n- python3/store.py",
        "gene_type": "principle"
      },
      {
        "title": "python3 æ–‡ä»¶ç»“æ„",
        "content": "python3/Pipfile\npython3/Pipfile.lock\npython3/store.py",
        "gene_type": "pattern"
      }
    ],
    "version": 1,
    "usage_count": 0,
    "rating": 3.539078074370205
  },
  {
    "name": "model.nvim/queries æ¨¡å—",
    "description": "[æ¨¡å—çº§] model.nvim é¡¹ç›®çš„ queries æ¨¡å— (source)ã€‚æ¥æº: https://github.com/gsuuon/model.nvim",
    "domain": "ai-llm",
    "tags": [
      "ai",
      "chatgpt",
      "llm",
      "neovim",
      "neovim-plugin",
      "module-level",
      "source",
      "queries"
    ],
    "genes": [
      {
        "title": "queries æ¨¡å—è¯´æ˜",
        "content": "æ¨¡å—è·¯å¾„: queries/\nç±»å‹: source\næ–‡ä»¶æ•°: 4\n\nåŒ…å«æ–‡ä»¶:\n- queries/mchat\n- queries/mchat/folds.scm\n- queries/mchat/highlights.scm\n- queries/mchat/injections.scm",
        "gene_type": "principle"
      },
      {
        "title": "queries æ–‡ä»¶ç»“æ„",
        "content": "queries/mchat\nqueries/mchat/folds.scm\nqueries/mchat/highlights.scm\nqueries/mchat/injections.scm",
        "gene_type": "pattern"
      }
    ],
    "version": 1,
    "usage_count": 0,
    "rating": 3.539078074370205
  }
]